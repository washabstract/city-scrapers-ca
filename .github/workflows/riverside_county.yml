name: Scrape County of Riverside

on:
  schedule:
    # Set any time that you'd like scrapers to run (in UTC)
    - cron: "1 6 * * *"
  workflow_dispatch:

env:
  CI: true
  PIPENV_VENV_IN_PROJECT: true
  SCRAPY_SETTINGS_MODULE: city_scrapers.settings.prod
  WAYBACK_ENABLED: true
  AUTOTHROTTLE_MAX_DELAY: 30.0
  AUTOTHROTTLE_START_DELAY: 1.5
  AUTOTHROTTLE_TARGET_CONCURRENCY: 3.0
  AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
  AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
  S3_BUCKET: "s3://wa-city-scrapers-store/riverside/"
  SENTRY_DSN: "https://e9f961e3177d4573a5e1bea7d4d552f1@o488083.ingest.sentry.io/4504601086984192"
  POSTGRES_DATABASE: "cityscrapers"
  POSTGRES_HOST: "openstates-production.cfd4ggfnyky4.us-west-1.rds.amazonaws.com"
  POSTGRES_PASSWORD: "lsjrdTmE7t8FU0nqfhhE"
  POSTGRES_USER: "postgres"


jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2

      - name: Set up Python 3.8
        uses: actions/setup-python@v1
        with:
          python-version: 3.8

      - name: Install Pipenv
        uses: dschep/install-pipenv-action@v1

      - name: Install dependencies
        run: pipenv sync
        env:
          PIPENV_DEFAULT_PYTHON_VERSION: 3.8

      - name: Run scrapers
        run: |
          export PYTHONPATH=$(pwd):$PYTHONPATH
          pipenv run scrapy crawl riverside_county
      - name: Combine output feeds
        run: |
          export PYTHONPATH=$(pwd):$PYTHONPATH
          pipenv run scrapy combinefeeds -s LOG_ENABLED=False
